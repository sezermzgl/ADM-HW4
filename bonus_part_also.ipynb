{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0igxiT90i_l_"
   },
   "source": [
    "# **2.1 Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwJPSHlXZw2l",
    "outputId": "ea7b9b60-f6cd-41fb-9fb7-d3fed34b7cd3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "movies = pd.read_csv('movie.csv')\n",
    "ratings = pd.read_csv('rating.csv')\n",
    "genome_scores = pd.read_csv('genome_scores.csv')\n",
    "genome_tags = pd.read_csv('genome_tags.csv')\n",
    "\n",
    "# Display the first few rows of each file to understand the structure\n",
    "print(\"Movies:\")\n",
    "print(movies.head())\n",
    "print(\"\\nRatings:\")\n",
    "print(ratings.head())\n",
    "print(\"\\nGenome Scores:\")\n",
    "print(genome_scores.head())\n",
    "print(\"\\nGenome Tags:\")\n",
    "print(genome_tags.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEJh_Ojre_B6",
    "outputId": "bdf3f40b-9118-4419-b7d1-09b3aea2730f"
   },
   "outputs": [],
   "source": [
    "# Calculate average rating for each movie\n",
    "movie_avg_ratings = ratings.groupby('movieId')['rating'].mean().reset_index()\n",
    "movie_avg_ratings.rename(columns={'rating': 'ratings_avg'}, inplace=True)\n",
    "\n",
    "# Merge the average ratings back to the movies DataFrame\n",
    "movies = movies.merge(movie_avg_ratings, on='movieId', how='left')\n",
    "\n",
    "print(\"Movies with average ratings:\")\n",
    "print(movies.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFeFg6pLfE_p",
    "outputId": "ca26375e-fd71-4c8c-adde-2bc1968bec83"
   },
   "outputs": [],
   "source": [
    "# Split genres into binary features\n",
    "genre_columns = movies['genres'].str.get_dummies(sep='|')\n",
    "\n",
    "# Add the genre columns to the movies DataFrame\n",
    "movies = pd.concat([movies, genre_columns], axis=1)\n",
    "\n",
    "print(\"Movies with genre features:\")\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXUehu8tgYZd",
    "outputId": "21e7fd31-6a7e-4100-9a0f-b41df5730a98"
   },
   "outputs": [],
   "source": [
    "# Merge genome_scores with genome_tags to get tag names\n",
    "genome_data = genome_scores.merge(genome_tags, on='tagId', how='left')\n",
    "\n",
    "# Find the most relevant tag for each movie\n",
    "relevant_tags = genome_data.loc[genome_data.groupby('movieId')['relevance'].idxmax()]\n",
    "relevant_tags = relevant_tags[['movieId', 'tag']].rename(columns={'tag': 'relevant_genome_tag'})\n",
    "\n",
    "# Merge the relevant tags back to the movies DataFrame\n",
    "movies = movies.merge(relevant_tags, on='movieId', how='left')\n",
    "\n",
    "print(\"Movies with relevant tags:\")\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaALF9vGgk4m",
    "outputId": "6fcd23ff-9369-48df-f2f1-2d4ce4bf7f86"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of ratings for each movie\n",
    "movie_num_ratings = ratings.groupby('movieId')['rating'].count().reset_index()\n",
    "movie_num_ratings.rename(columns={'rating': 'num_ratings'}, inplace=True)\n",
    "\n",
    "# Merge the number of ratings back to the movies DataFrame\n",
    "movies = movies.merge(movie_num_ratings, on='movieId', how='left')\n",
    "\n",
    "print(\"Movies with number of ratings:\")\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLDrFym_guCz",
    "outputId": "845a356e-a4b6-4a8f-b7c0-03235af60f51"
   },
   "outputs": [],
   "source": [
    "# Load tags data\n",
    "tags = pd.read_csv('tag.csv')\n",
    "\n",
    "# Find the most common tag for each movie\n",
    "common_tags = tags.groupby('movieId')['tag'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "common_tags.rename(columns={'tag': 'common_user_tag'}, inplace=True)\n",
    "\n",
    "# Merge the common tags back to the movies DataFrame\n",
    "movies = movies.merge(common_tags, on='movieId', how='left')\n",
    "\n",
    "print(\"Movies with common user tags:\")\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gD9P_H_Sg0AW",
    "outputId": "afb73d3f-6342-47a1-fa3b-fd90d1e5c671"
   },
   "outputs": [],
   "source": [
    "# Select relevant columns for clustering\n",
    "final_data = movies[['movieId', 'ratings_avg', 'num_ratings', 'relevant_genome_tag', 'common_user_tag'] + list(genre_columns.columns)]\n",
    "\n",
    "# Display the final data for clustering\n",
    "print(\"Final Data for Clustering:\")\n",
    "print(final_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGeHH2-49b7P",
    "outputId": "5ddde49d-a1f7-44e7-dcfb-a5b4f7175639"
   },
   "outputs": [],
   "source": [
    "# Add a new feature: Length of the movie title\n",
    "movies['title_length'] = movies['title'].apply(len)\n",
    "\n",
    "# Include this feature in the final dataset\n",
    "final_data['title_length'] = movies['title_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another new feature\n",
    "movies['num_genres'] = movies['genres'].apply(lambda x: len(x.split('|')))\n",
    "\n",
    "# Include this feature in the final dataset\n",
    "final_data['num_genres'] = movies['num_genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another new feature\n",
    "movies['avg_rating_per_genre'] = movies.groupby('genres')['ratings_avg'].transform('mean')\n",
    "\n",
    "# Include this feature in the final dataset\n",
    "final_data['avg_rating_per_genre'] = movies['avg_rating_per_genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etR_eA9oiwjg"
   },
   "source": [
    "We achieved more than eight features because we represented each genre as a separate binary feature. Since movies can belong to multiple genres, each unique genre contributes a separate column. Additionally, we included other derived features such as the number of ratings, average rating, most relevant genome tag, most common user tag, title length, and more. Together, these exceed eight features, allowing for a more robust clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nez8P0NzmAQo"
   },
   "source": [
    "# **2.2 Choose your features (variables)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCEQyQx6mUeY"
   },
   "source": [
    "**Standardize the data**\n",
    "\n",
    "Clustering algorithms, such as K-means, are sensitive to the scale of features. If some features (e.g., num_ratings) have a larger range of values, they will dominate over features with smaller ranges (e.g., ratings_avg), which can distort the results.\n",
    "\n",
    "We standardize numerical features, such as ratings_avg and num_ratings, to ensure all features contribute equally to the outcome.\n",
    "\n",
    "We will use StandardScaler from the sklearn library to standardize the data (bringing the mean to 0 and the standard deviation to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03gUTtFamEXK",
    "outputId": "9054b9b5-9810-4a87-e413-258615665765"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Select numeric features for normalization\n",
    "numeric_features = ['ratings_avg', 'num_ratings']\n",
    "categorical_features = ['relevant_genome_tag', 'common_user_tag']\n",
    "genre_features = list(genre_columns)\n",
    "additional_features = ['title_length', 'num_genres', 'avg_rating_per_genre']\n",
    "\n",
    "# Combine all features for clustering\n",
    "final_features = numeric_features + genre_features + additional_features\n",
    "data_for_clustering = final_data[final_features]\n",
    "\n",
    "# Step 2: Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "data_for_clustering[numeric_features] = scaler.fit_transform(data_for_clustering[numeric_features])\n",
    "data_for_clustering[additional_features] = scaler.fit_transform(data_for_clustering[additional_features])\n",
    "data_for_clustering[genre_features] = scaler.fit_transform(data_for_clustering[genre_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1faCbWgZmv5J"
   },
   "source": [
    "**Dimensionality Reduction**\n",
    "\n",
    "If we have too many features, it can increase computational complexity and make interpreting the results more challenging.\n",
    "We will use PCA (Principal Component Analysis) to reduce the number of features while preserving the most important information. PCA will decrease the number of features, preparing the data for the next clustering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOpT6Pe4_OwV",
    "outputId": "3fb69f79-2e99-4ecb-85bb-df22d13717f5"
   },
   "outputs": [],
   "source": [
    "print(data_for_clustering.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "872tBUbl_kKj",
    "outputId": "22cf5a91-76c9-432d-9b72-981760389449"
   },
   "outputs": [],
   "source": [
    "data_for_clustering['ratings_avg'].fillna(data_for_clustering['ratings_avg'].median(), inplace=True)\n",
    "data_for_clustering['num_ratings'].fillna(data_for_clustering['num_ratings'].median(), inplace=True)\n",
    "data_for_clustering['avg_rating_per_genre'].fillna(data_for_clustering['avg_rating_per_genre'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sSYBx8g_mJr",
    "outputId": "2725e71a-593b-4c84-d8ce-f5be14ae31e7"
   },
   "outputs": [],
   "source": [
    "print(data_for_clustering.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DePK_cN_nXNI",
    "outputId": "0f1e244a-48d6-458a-991a-db6482ca26f8"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 3: Perform dimensionality reduction using PCA\n",
    "pca = PCA(n_components=10)  # Reduce to 10 components for simplicity\n",
    "pca_data = pca.fit_transform(data_for_clustering)\n",
    "\n",
    "# Step 4: Analyze explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance ratio by each PCA component: {explained_variance}\")\n",
    "print(f\"Cumulative explained variance: {explained_variance.cumsum()}\")\n",
    "\n",
    "# Step 5: Save transformed data\n",
    "data_pca = pd.DataFrame(pca_data, columns=[f'PC{i+1}' for i in range(pca_data.shape[1])])\n",
    "\n",
    "print(\"Transformed data after PCA:\")\n",
    "print(data_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio) \n",
    "\n",
    "# Visualizing in a graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', label='Cumulative')\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, label='Explained Variance')\n",
    "\n",
    "plt.title('Explained and cumulative variance by principal component', fontsize=14)\n",
    "plt.xlabel('Number of principal components', fontsize=12)\n",
    "plt.ylabel('Explained variace', fontsize=12)\n",
    "plt.xticks(range(1, len(cumulative_variance) + 1))\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Compute the loadings\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  \n",
    "    columns=[f\"PC{i+1}\" for i in range(pca.components_.shape[0])],\n",
    "    index = data_for_clustering.columns\n",
    ")\n",
    "\n",
    "# Print loadings\n",
    "print(\"Loadings (incidence of variables for each PC):\")\n",
    "print(loadings)\n",
    "num_cp = 5\n",
    "# Visualization of loadings fro the first 2 principal components\n",
    "plt.figure(figsize=(10, 6))\n",
    "loadings.iloc[:, :num_cp].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Loadings of variables for the first 2 PC\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Loadings value\")\n",
    "plt.axhline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "plt.legend([f\"PC{i+1}\" for i in range(num_cp)])  # Aggiorna automaticamente la legenda\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this analysis we can choose the optimal number of principal components to use for our cluster analysis. In that case, a good number of components can me 5. The first reason is that between the 5th and the 6th compontent there is the last decrease of explained variance, even if its tiny in that contex is enough. Last reason is that with 5 PC we manage to have the 40% of the variance explained. It's a quite low value, but it's quite normal working with a very big number of features, most of which have no internal correlations. For this reason, when applying a dimensional reduction it is almost mandatory to lose a large part of the information. 5 principal components is a good compromise having an acceptable information retained, still obtaining a great dimensional reduction of the dataset, facilitating the work in cluster analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1wI6M0XN9yi"
   },
   "source": [
    "# **2.3 Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDb3KGIeP8Xg"
   },
   "source": [
    "**Optimal number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_gzWIAVFN-Lt",
    "outputId": "48929cf6-d87c-410f-8570-e7c982cbdffb"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Elbow Method\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "cluster_range = range(2, 11)\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_pca)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(data_pca, kmeans.labels_))\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cluster_range, inertia, marker='o', label='Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o', label='Silhouette Score', color='orange')\n",
    "plt.title('Silhouette Scores')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsH8puJlQtOj"
   },
   "source": [
    "**Elbow Method:**\n",
    "The inertia graph shows a sharp decrease in inertia values as the number of clusters increases, especially up to 4 clusters. After that, the rate of decrease slows significantly.\n",
    "This suggests that the optimal number of clusters is likely around 4, as adding more clusters beyond this point does not significantly improve the model (inertia reduction becomes minimal).\n",
    "\n",
    "**Silhouette Scores:**\n",
    "The silhouette coefficient reaches its maximum at 2 clusters, then gradually decreases as the number of clusters increases.\n",
    "However, having only 2 clusters might be too generalized and could miss important details in the data. Values for 3 and 4 clusters remain relatively high, indicating that these are good choices for the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNtMSnVSQ0_I"
   },
   "source": [
    "**K-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zwp2wvmPQ67O",
    "outputId": "65606582-6c9d-41eb-98d1-b86f0b9ba546"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize cluster centers randomly\n",
    "def initialize_centers(data, k):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    random_indices = np.random.choice(data.shape[0], size=k, replace=False)\n",
    "    return data[random_indices]\n",
    "\n",
    "# Assign points to the nearest cluster center\n",
    "def assign_clusters(data, centers):\n",
    "    distances = np.linalg.norm(data[:, np.newaxis] - centers, axis=2)\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "# Update cluster centers\n",
    "def update_centers(data, labels, k):\n",
    "    new_centers = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
    "    return new_centers\n",
    "\n",
    "# K-means algorithm using MapReduce logic\n",
    "def kmeans(data, k, max_iters=100, tol=1e-4):\n",
    "    centers = initialize_centers(data, k)\n",
    "    for i in range(max_iters):\n",
    "        labels = assign_clusters(data, centers)\n",
    "        new_centers = update_centers(data, labels, k)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.all(np.abs(new_centers - centers) < tol):\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "        centers = new_centers\n",
    "    return centers, labels\n",
    "\n",
    "# Run K-means with chosen k\n",
    "k = 13  # Set the number of clusters (from elbow method)\n",
    "data = data_pca.values  # Convert PCA-transformed data to NumPy array\n",
    "\n",
    "centers, labels = kmeans(data, k)\n",
    "\n",
    "# Output results\n",
    "print(\"Final cluster centers:\")\n",
    "print(centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate Sum of Squared Distances (SSD)\n",
    "def compute_ssd(data, centers, labels):\n",
    "    ssd = 0\n",
    "    for i, center in enumerate(centers):\n",
    "        cluster_points = data[labels == i]\n",
    "        ssd += np.sum((cluster_points - center) ** 2)\n",
    "    return ssd\n",
    "\n",
    "# Function to run the elbow method\n",
    "def elbow_method(data, max_k=10):\n",
    "    ssd_values = []\n",
    "    k_values = range(1, max_k + 1)\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"Running K-means for k={k}...\")\n",
    "        centers, labels = kmeans(data, k)\n",
    "        ssd = compute_ssd(data, centers, labels)\n",
    "        ssd_values.append(ssd)\n",
    "\n",
    "    return k_values, ssd_values\n",
    "\n",
    "# Plotting the elbow curve\n",
    "def plot_elbow_curve(k_values, ssd_values):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(k_values, ssd_values, marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Number of Clusters (k)\")\n",
    "    plt.ylabel(\"Sum of Squared Distances (SSD)\")\n",
    "    plt.title(\"Elbow Method for Optimal k\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# PCA-transformed data\n",
    "data = data_pca.values  # Ensure PCA data is a NumPy array\n",
    "\n",
    "# Run elbow method\n",
    "k_values, ssd_values = elbow_method(data, max_k=10)\n",
    "\n",
    "# Plot the results\n",
    "plot_elbow_curve(k_values, ssd_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79nvrMLjTvPC",
    "outputId": "dd78f240-8ec7-4e01-80f8-212e472f105d"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cluster_counts = Counter(labels)\n",
    "print(\"Number of points in each cluster:\")\n",
    "for cluster_id, count in sorted(cluster_counts.items(), key=lambda item: item[1]):\n",
    "    print(f\"Cluster {cluster_id}: {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Plot clusters in 3D\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# First plot: PC1, PC2, PC3\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(pca_data[:, 0], pca_data[:, 1], pca_data[:, 2], c=labels, cmap='viridis')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_zlabel('PC3')\n",
    "ax1.set_title('Clustering 3D - PC1, PC2, PC3')\n",
    "\n",
    "# Second plot: PC2, PC3, PC4\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(pca_data[:, 1], pca_data[:, 2], pca_data[:, 3], c=labels, cmap='viridis')\n",
    "ax2.set_xlabel('PC2')\n",
    "ax2.set_ylabel('PC3')\n",
    "ax2.set_zlabel('PC4')\n",
    "ax2.set_title('Clustering 3D - PC2, PC3, PC4')\n",
    "\n",
    "# Third plot: PC3, PC4, PC5\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.scatter(pca_data[:, 2], pca_data[:, 3], pca_data[:, 4], c=labels, cmap='viridis')\n",
    "ax3.set_xlabel('PC3')\n",
    "ax3.set_ylabel('PC4')\n",
    "ax3.set_zlabel('PC5')\n",
    "ax3.set_title('Clustering 3D - PC3, PC4, PC5')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].scatter(data_pca['PC1'], data_pca['PC2'], c=labels, cmap='viridis')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[1].scatter(data_pca['PC1'], data_pca['PC3'], c=labels, cmap='viridis')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC3')\n",
    "axes[2].scatter(data_pca['PC2'], data_pca['PC3'], c=labels, cmap='viridis')\n",
    "axes[2].set_xlabel('PC2')\n",
    "axes[2].set_ylabel('PC3')\n",
    "plt.suptitle('Clustering 2D - Principal couples of PC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFhBhD5VW_fh"
   },
   "source": [
    "**K-means++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buKFuWckW_4w",
    "outputId": "c4020e95-782a-4814-8293-77080dfed4d8"
   },
   "outputs": [],
   "source": [
    "def kmeans_pp(data, k, max_iter=300, tol=1e-4):\n",
    "    n_samples, _ = data.shape\n",
    "    centroids = []\n",
    "\n",
    "    # Initialize centroids using K-means++\n",
    "    first_idx = np.random.randint(0, n_samples)\n",
    "    centroids.append(data[first_idx])\n",
    "\n",
    "    for _ in range(1, k):\n",
    "        distances = np.min([np.linalg.norm(data - c, axis=1) ** 2 for c in centroids], axis=0)\n",
    "        probabilities = distances / distances.sum()\n",
    "        next_idx = np.random.choice(range(n_samples), p=probabilities)\n",
    "        centroids.append(data[next_idx])\n",
    "\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    for _ in range(max_iter):\n",
    "        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
    "\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, labels\n",
    "\n",
    "# Apply K-means++ to data_pca\n",
    "k = 3\n",
    "centroids_pp, labels_pp = kmeans_pp(data_pca.values, k)\n",
    "\n",
    "# Analyze cluster sizes\n",
    "from collections import Counter\n",
    "cluster_counts_pp = Counter(labels_pp)\n",
    "print(\"Cluster sizes with K-means++:\")\n",
    "for cluster_id, count in cluster_counts_pp.items():\n",
    "    print(f\"Cluster {cluster_id}: {count} points\")\n",
    "\n",
    "# Print cluster centers\n",
    "print(\"\\nCluster centers with K-means++:\")\n",
    "print(centroids_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# First plot: PC1, PC2, PC3\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(pca_data[:, 0], pca_data[:, 1], pca_data[:, 2], c=labels_pp, cmap='viridis')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_zlabel('PC3')\n",
    "ax1.set_title('Clustering 3D - PC1, PC2, PC3')\n",
    "\n",
    "# Second plot: PC2, PC3, PC4\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(pca_data[:, 1], pca_data[:, 2], pca_data[:, 3], c=labels_pp, cmap='viridis')\n",
    "ax2.set_xlabel('PC2')\n",
    "ax2.set_ylabel('PC3')\n",
    "ax2.set_zlabel('PC4')\n",
    "ax2.set_title('Clustering 3D - PC2, PC3, PC4')\n",
    "\n",
    "# Third plot: PC3, PC4, PC5\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.scatter(pca_data[:, 2], pca_data[:, 3], pca_data[:, 4], c=labels_pp, cmap='viridis')\n",
    "ax3.set_xlabel('PC3')\n",
    "ax3.set_ylabel('PC4')\n",
    "ax3.set_zlabel('PC5')\n",
    "ax3.set_title('Clustering 3D - PC3, PC4, PC5')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].scatter(data_pca['PC1'], data_pca['PC2'], c=labels_pp, cmap='viridis')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[1].scatter(data_pca['PC1'], data_pca['PC3'], c=labels_pp, cmap='viridis')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC3')\n",
    "axes[2].scatter(data_pca['PC2'], data_pca['PC3'], c=labels_pp, cmap='viridis')\n",
    "axes[2].set_xlabel('PC2')\n",
    "axes[2].set_ylabel('PC3')\n",
    "plt.suptitle('Clustering 2D - Principal couples of PC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7lRaqwpYfz_"
   },
   "source": [
    "The cluster sizes remain the same, indicating that both methods arrived at the same distribution of points across clusters.\n",
    "\n",
    "\n",
    "The cluster centers for K-means++ differ from those of regular K-means. This is due to the difference in initialization methods. In K-means++, the initial centers are selected to minimize the likelihood of a poor choice of starting points, which typically leads to more stable and optimal results.\n",
    "Regular K-means uses random initialization, which can lead to varying results across different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYogtsp3aSm6"
   },
   "source": [
    "**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "\n",
    "1. The algorithm uses two main parameters:\n",
    "\n",
    "* eps (epsilon): the maximum distance between two points for them to be considered in the same cluster.\n",
    "* min_samples: the minimum number of points required within the eps radius for a point to be considered a \"core\" point.\n",
    "\n",
    "2. Points are categorized into three types:\n",
    "\n",
    "* Core points: have at least min_samples neighbors within the eps radius.\n",
    "\n",
    "* Border points: are within the eps radius of a core point but do not themselves have enough neighbors to be considered core points.\n",
    "\n",
    "* Noise points: do not belong to any cluster and are considered outliers.\n",
    "\n",
    "3. Clusters are formed around core points. Border points are assigned to the nearest cluster. Noise points remain unclustered.\n",
    "\n",
    "**Advantages of DBSCAN:**\n",
    "\n",
    "* Works well with clusters of arbitrary shapes.\n",
    "* Handles noise (outliers) effectively.\n",
    "* Does not require specifying the number of clusters in advance.\n",
    "\n",
    "**Disadvantages of DBSCAN:**\n",
    "\n",
    "* Sensitive to the choice of eps and min_samples parameters.\n",
    "* Struggles with high-dimensional data (due to increased computational complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRlz964m40vb",
    "outputId": "ada68236-9445-424e-d6f0-bd548b589d20"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Set DBSCAN parameters\n",
    "eps_value = 0.5\n",
    "min_samples_value = 5\n",
    "\n",
    "# Apply DBSCAN to data_pca\n",
    "dbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)\n",
    "dbscan_labels = dbscan.fit_predict(data_pca.values)\n",
    "\n",
    "# Analyze results\n",
    "unique_clusters = np.unique(dbscan_labels)\n",
    "print(f\"Number of clusters (including noise): {len(unique_clusters)}\")\n",
    "print(f\"Cluster labels: {unique_clusters}\")\n",
    "\n",
    "# Silhouette score\n",
    "if len(unique_clusters) > 1:\n",
    "    silhouette_avg = silhouette_score(data_pca.values[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
    "    print(f\"Average silhouette score (excluding noise): {silhouette_avg:.3f}\")\n",
    "else:\n",
    "    print(\"Silhouette score not applicable (only one cluster).\")\n",
    "\n",
    "# Cluster sizes\n",
    "from collections import Counter\n",
    "cluster_counts = Counter(dbscan_labels)\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster_id}: {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEJIqkw4ypZT",
    "outputId": "e72cff74-ab73-4da1-f9c8-1e09643879a6"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Set DBSCAN parameters\n",
    "eps_value = 1.0\n",
    "min_samples_value = 5\n",
    "\n",
    "# Apply DBSCAN to data_pca\n",
    "dbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)\n",
    "dbscan_labels = dbscan.fit_predict(data_pca.values)\n",
    "\n",
    "# Analyze results\n",
    "unique_clusters = np.unique(dbscan_labels)\n",
    "print(f\"Number of clusters (including noise): {len(unique_clusters)}\")\n",
    "print(f\"Cluster labels: {unique_clusters}\")\n",
    "\n",
    "# Silhouette score\n",
    "if len(unique_clusters) > 1:\n",
    "    silhouette_avg = silhouette_score(data_pca.values[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
    "    print(f\"Average silhouette score (excluding noise): {silhouette_avg:.3f}\")\n",
    "else:\n",
    "    print(\"Silhouette score not applicable (only one cluster).\")\n",
    "\n",
    "# Cluster sizes\n",
    "from collections import Counter\n",
    "cluster_counts = Counter(dbscan_labels)\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster_id}: {count} points\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI3Oad6nAWtg"
   },
   "source": [
    "### **2.4 Best Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOx-j5TIAdjb",
    "outputId": "4348ff9b-292c-4e82-9f23-8ba238b83586"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Inertia calculation\n",
    "def calculate_inertia(data, labels, centers):\n",
    "    inertia = 0\n",
    "    for i, center in enumerate(centers):\n",
    "        cluster_points = data[labels == i]\n",
    "        inertia += np.sum((cluster_points - center) ** 2)\n",
    "    return inertia\n",
    "\n",
    "# K-means Results\n",
    "print(\"K-means Results:\")\n",
    "data_array = data_pca.values  # Convert DataFrame to NumPy array\n",
    "inertia_kmeans = calculate_inertia(data_array, labels, centers)\n",
    "silhouette_kmeans = silhouette_score(data_array, labels)\n",
    "davies_bouldin_kmeans = davies_bouldin_score(data_array, labels)\n",
    "print(f\"Inertia for K-means: {inertia_kmeans}\")\n",
    "print(f\"Silhouette Score for K-means: {silhouette_kmeans}\")\n",
    "print(f\"Davies-Bouldin Index for K-means: {davies_bouldin_kmeans}\")\n",
    "\n",
    "# K-means++ Results\n",
    "print(\"\\nK-means++ Results:\")\n",
    "inertia_kmeans_pp = calculate_inertia(data_array, labels_pp, centroids_pp)\n",
    "silhouette_kmeans_pp = silhouette_score(data_array, labels_pp)\n",
    "davies_bouldin_kmeans_pp = davies_bouldin_score(data_array, labels_pp)\n",
    "print(f\"Inertia for K-means++: {inertia_kmeans_pp}\")\n",
    "print(f\"Silhouette Score for K-means++: {silhouette_kmeans_pp}\")\n",
    "print(f\"Davies-Bouldin Index for K-means++: {davies_bouldin_kmeans_pp}\")\n",
    "\n",
    "# DBSCAN Results\n",
    "print(\"\\nDBSCAN Results:\")\n",
    "unique_clusters = np.unique(dbscan_labels)\n",
    "if len(unique_clusters) > 1:  # Check if DBSCAN found multiple clusters\n",
    "    silhouette_dbscan = silhouette_score(data_array[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
    "    davies_bouldin_dbscan = davies_bouldin_score(data_array[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
    "    print(f\"Silhouette Score for DBSCAN: {silhouette_dbscan}\")\n",
    "    print(f\"Davies-Bouldin Index for DBSCAN: {davies_bouldin_dbscan}\")\n",
    "else:\n",
    "    silhouette_dbscan = None\n",
    "    davies_bouldin_dbscan = None\n",
    "    print(\"Silhouette Score and Davies-Bouldin Index are not applicable for DBSCAN (only one cluster detected).\")\n",
    "\n",
    "# Summary of Results\n",
    "print(\"\\nSummary of Clustering Metrics:\")\n",
    "print(f\"K-means:\\n - Inertia: {inertia_kmeans}\\n - Silhouette Score: {silhouette_kmeans}\\n - Davies-Bouldin Index: {davies_bouldin_kmeans}\")\n",
    "print(f\"K-means++:\\n - Inertia: {inertia_kmeans_pp}\\n - Silhouette Score: {silhouette_kmeans_pp}\\n - Davies-Bouldin Index: {davies_bouldin_kmeans_pp}\")\n",
    "if silhouette_dbscan is not None:\n",
    "    print(f\"DBSCAN:\\n - Silhouette Score: {silhouette_dbscan}\\n - Davies-Bouldin Index: {davies_bouldin_dbscan}\")\n",
    "else:\n",
    "    print(\"DBSCAN metrics are not available due to insufficient clustering.\")\n",
    "\n",
    "# Cluster Size Distribution\n",
    "from collections import Counter\n",
    "print(\"\\nCluster Size Distribution:\")\n",
    "print(\"K-means:\", Counter(labels))\n",
    "print(\"K-means++:\", Counter(labels_pp))\n",
    "print(\"DBSCAN:\", Counter(dbscan_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ6Tb-4049m0"
   },
   "source": [
    "Clustering is a powerful tool to identify natural groupings within data. In this section, we evaluated the quality of clustering results using three algorithms: K-means, K-means++, and DBSCAN. Our goal was to determine the most suitable algorithm for the given dataset. Below are the detailed steps, metrics, and analysis:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Optimal Number of Clusters (\\( k_{opt} \\))**\n",
    "Based on the results from the Elbow Method and Silhouette Analysis in section 2.3, we determined that the optimal number of clusters for K-means and K-means++ is \\( k_opt = 4 \\).\n",
    "\n",
    "For DBSCAN, the optimal parameters were identified as:\n",
    "- \\( eps = 1 )\n",
    "- \\( min\\_samples = 5 )\n",
    "\n",
    "These parameters were used to compare the three clustering methods.\n",
    "\n",
    "\n",
    "#### **Metrics for Evaluating Clustering Quality**\n",
    "We used the following metrics to assess the quality of clustering:\n",
    "1. **Inertia**: The sum of squared distances from points to the cluster centers. Lower values indicate more compact clusters.\n",
    "2. **Silhouette Score**: Measures how well clusters are separated and how similar points are within the same cluster. Higher values indicate better-defined clusters.\n",
    "3. **Davies-Bouldin Index (DBI)**: Reflects the average similarity between each cluster and the one most similar to it. Lower values are better.\n",
    "4. **Cluster Size Distribution**: Examines the balance in cluster sizes. Balanced clusters are generally desirable unless the data has inherent imbalances.\n",
    "\n",
    "\n",
    "#### **Applying and Evaluating the Algorithms**\n",
    "\n",
    "##### **1. K-means**\n",
    "- **Number of clusters**: 4 (predefined as \\( k_opt \\)).\n",
    "- **Metrics**:\n",
    "  - **Inertia**: 769,611.93\n",
    "  - **Silhouette Score**: 0.485\n",
    "  - **Davies-Bouldin Index**: 0.633\n",
    "- **Cluster sizes**:\n",
    "  - Cluster 0: 1,364 points\n",
    "  - Cluster 1: 10,074 points\n",
    "  - Cluster 2: 10,954 points\n",
    "  - Cluster 3: 4,886 points\n",
    "- **Observations**:\n",
    "  - Clusters are moderately well-separated with a balanced distribution.\n",
    "  - Silhouette Score indicates moderately good separation and compactness.\n",
    "\n",
    "\n",
    "##### **2. K-means++**\n",
    "- **Number of clusters**: 4 (predefined as \\( k_opt \\)).\n",
    "- **Metrics**:\n",
    "  - **Inertia**: 782,496.54\n",
    "  - **Silhouette Score**: 0.531\n",
    "  - **Davies-Bouldin Index**: 0.582\n",
    "- **Cluster sizes**:\n",
    "  - Cluster 0: 7,200 points\n",
    "  - Cluster 1: 16,388 points\n",
    "  - Cluster 2: 658 points\n",
    "  - Cluster 3: 3,032 points\n",
    "- **Observations**:\n",
    "  - K-means++ achieved better initialization compared to K-means, leading to slightly better compactness and separation.\n",
    "  - Cluster sizes are more imbalanced compared to K-means.\n",
    "\n",
    "\n",
    "##### **3. DBSCAN**\n",
    "- **Number of clusters**: 31 (including noise).\n",
    "- **Metrics**:\n",
    "  - **Silhouette Score**: 0.302\n",
    "  - **Davies-Bouldin Index**: 1.017\n",
    "- **Cluster sizes**:\n",
    "  - Dominant Cluster 0: 24,194 points\n",
    "  - Noise (-1): 2,772 points\n",
    "  - Remaining clusters: Mostly small, with less than 100 points each.\n",
    "- **Observations**:\n",
    "  - DBSCAN identified one dominant cluster and many smaller ones, indicating it is not well-suited for this dataset.\n",
    "  - The negative silhouette score suggests overlapping or poorly separated clusters.\n",
    "  - DBSCAN’s performance is sensitive to parameter selection.\n",
    "\n",
    "\n",
    "#### **Comparing Results**\n",
    "\n",
    "| Algorithm    | Inertia    | Silhouette Score | Davies-Bouldin Index | Number of Clusters | Dominant Cluster Size | Observations                                     |\n",
    "|--------------|------------|------------------|-----------------------|---------------------|-----------------------|-------------------------------------------------|\n",
    "| **K-means**  | 769,611.93 | 0.485            | 0.633                 | 4                   | 10,954 points         | Balanced clusters, moderately good separation. |\n",
    "| **K-means++**| 782,496.54 | 0.531            | 0.582                 | 4                   | 16,388 points         | Faster convergence, more imbalanced clusters.  |\n",
    "| **DBSCAN**   | N/A        | 0.302            | 1.017                 | 31                  | 24,194 points         | Poor separation, unsuitable for this dataset.  |\n",
    "\n",
    "\n",
    "#### **Conclusion**\n",
    "Based on the evaluation:\n",
    "1. **K-means++** emerged as the most suitable algorithm, achieving the highest Silhouette Score and lowest Davies-Bouldin Index, indicating well-defined and compact clusters.\n",
    "2. **K-means** performed slightly worse than K-means++ but produced more balanced clusters, making it a viable alternative depending on the application.\n",
    "3. **DBSCAN** struggled with the dataset, identifying one dominant cluster and many small, poorly separated clusters. This method is unsuitable for this dataset without significant parameter tuning.\n",
    "\n",
    "Overall, **K-means++** is recommended for its superior clustering quality, but **K-means** may be preferred if cluster balance is critical. DBSCAN is not recommended for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 by better pair of PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from itertools import combinations\n",
    "\n",
    "# Numero di cluster\n",
    "n_clusters = 3\n",
    "\n",
    "# Prepara la matrice delle componenti principali\n",
    "data_pca_np = data_pca.to_numpy()\n",
    "\n",
    "# Per tutte le combinazioni di due componenti principali\n",
    "scores = []  # Per memorizzare silhouette e Davies-Bouldin scores per ogni coppia\n",
    "graphs_data = []  # Per memorizzare i dati per i grafici migliori\n",
    "\n",
    "for pair in combinations(range(data_pca_np.shape[1]), 2):\n",
    "    pc_x, pc_y = pair\n",
    "    selected_data = data_pca_np[:, [pc_x, pc_y]]  # Seleziona due componenti\n",
    "\n",
    "    # Esegui il K-means su queste due componenti\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(selected_data)\n",
    "\n",
    "    # Calcola i punteggi\n",
    "    silhouette = silhouette_score(selected_data, kmeans_labels)\n",
    "    davies_bouldin = davies_bouldin_score(selected_data, kmeans_labels)\n",
    "\n",
    "    # Memorizza i risultati\n",
    "    scores.append((pair, silhouette, davies_bouldin))\n",
    "    graphs_data.append((pair, selected_data, kmeans_labels, kmeans.cluster_centers_, silhouette, davies_bouldin))\n",
    "\n",
    "# Ordina le coppie per silhouette score (decrescente) e Davies-Bouldin index (crescente)\n",
    "top_3_silhouette = sorted(scores, key=lambda x: x[1], reverse=True)[:3]\n",
    "top_3_davies = sorted(scores, key=lambda x: x[2])[:3]\n",
    "\n",
    "# Funzione per visualizzare i grafici\n",
    "def plot_top_results(top_results, metric_name):\n",
    "    for idx, (pair, selected_data, labels, centroids, silhouette, davies_bouldin) in enumerate(\n",
    "            [graphs_data[i[0]] for i in top_results]):\n",
    "        pc_x, pc_y = pair\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(selected_data[:, 0], selected_data[:, 1], c=labels, cmap='viridis', alpha=0.6, s=50)\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "        plt.title(f'Top {idx + 1} ({metric_name}) - PC{pc_x + 1} vs PC{pc_y + 1}\\n'\n",
    "                  f'Silhouette: {silhouette:.2f}, Davies-Bouldin: {davies_bouldin:.2f}')\n",
    "        plt.xlabel(f'PC{pc_x + 1}')\n",
    "        plt.ylabel(f'PC{pc_y + 1}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Visualizza i grafici delle top 3 coppie per ciascun indice\n",
    "print(\"Top 3 pairs by Silhouette Score:\")\n",
    "plot_top_results([(scores.index(item), item[1]) for item in top_3_silhouette], \"Silhouette Score\")\n",
    "\n",
    "print(\"Top 3 pairs by Davies-Bouldin Index:\")\n",
    "plot_top_results([(scores.index(item), item[2]) for item in top_3_davies], \"Davies-Bouldin Index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or better if plots are side by side?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from itertools import combinations\n",
    "\n",
    "# Numero di cluster\n",
    "n_clusters = 3\n",
    "\n",
    "# Prepara la matrice delle componenti principali\n",
    "data_pca_np = data_pca.to_numpy()\n",
    "\n",
    "# Per tutte le combinazioni di due componenti principali\n",
    "scores = []  # Per memorizzare silhouette e Davies-Bouldin scores per ogni coppia\n",
    "graphs_data = []  # Per memorizzare i dati per i grafici migliori\n",
    "\n",
    "for pair in combinations(range(data_pca_np.shape[1]), 2):\n",
    "    pc_x, pc_y = pair\n",
    "    selected_data = data_pca_np[:, [pc_x, pc_y]]  # Seleziona due componenti\n",
    "\n",
    "    # Esegui il K-means su queste due componenti\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(selected_data)\n",
    "\n",
    "    # Calcola i punteggi\n",
    "    silhouette = silhouette_score(selected_data, kmeans_labels)\n",
    "    davies_bouldin = davies_bouldin_score(selected_data, kmeans_labels)\n",
    "\n",
    "    # Memorizza i risultati\n",
    "    scores.append((pair, silhouette, davies_bouldin))\n",
    "    graphs_data.append((pair, selected_data, kmeans_labels, kmeans.cluster_centers_, silhouette, davies_bouldin))\n",
    "\n",
    "# Ordina le coppie per silhouette score (decrescente) e Davies-Bouldin index (crescente)\n",
    "top_3_silhouette = sorted(scores, key=lambda x: x[1], reverse=True)[:3]\n",
    "top_3_davies = sorted(scores, key=lambda x: x[2])[:3]\n",
    "\n",
    "# Funzione per visualizzare i grafici affiancati\n",
    "def plot_top_results_side_by_side(top_results_silhouette, top_results_davies):\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # 3 righe, 2 colonne\n",
    "    \n",
    "    for row, (silhouette_item, davies_item) in enumerate(zip(top_results_silhouette, top_results_davies)):\n",
    "        for col, (item, metric_name, ax) in enumerate([\n",
    "                (silhouette_item, \"Silhouette Score\", axes[row, 0]),\n",
    "                (davies_item, \"Davies-Bouldin Index\", axes[row, 1])]):\n",
    "            \n",
    "            pair, selected_data, labels, centroids, silhouette, davies_bouldin = graphs_data[scores.index(item)]\n",
    "            pc_x, pc_y = pair\n",
    "            ax.scatter(selected_data[:, 0], selected_data[:, 1], c=labels, cmap='viridis', alpha=0.6, s=50)\n",
    "            ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "            ax.set_title(f'Top {row + 1} ({metric_name}) - PC{pc_x + 1} vs PC{pc_y + 1}\\n'\n",
    "                         f'Silhouette: {silhouette:.2f}, Davies-Bouldin: {davies_bouldin:.2f}')\n",
    "            ax.set_xlabel(f'PC{pc_x + 1}')\n",
    "            ax.set_ylabel(f'PC{pc_y + 1}')\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizza i grafici delle top 3 coppie per ciascun indice\n",
    "print(\"Visualizzazione dei grafici affiancati:\")\n",
    "plot_top_results_side_by_side(top_3_silhouette, top_3_davies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize iteration of kmeans for fixed steps or until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# Use only PC1 and PC2 for visualization\n",
    "selected_data = data_pca[['PC8', 'PC10']].to_numpy()\n",
    "\n",
    "# Parameters\n",
    "n_clusters = 7\n",
    "max_iter = 10\n",
    "\n",
    "# Random initialization of centroids\n",
    "rng = np.random.RandomState(42)\n",
    "initial_centroids = selected_data[rng.choice(selected_data.shape[0], n_clusters, replace=False)]\n",
    "\n",
    "centroids = initial_centroids\n",
    "all_iterations = []\n",
    "cluster_variances = []  # Store variances of individual clusters for each iteration\n",
    "\n",
    "for iteration in range(max_iter):\n",
    "    # Assign points to nearest cluster\n",
    "    labels = pairwise_distances_argmin(selected_data, centroids)\n",
    "\n",
    "    # Store cluster assignments for visualization\n",
    "    all_iterations.append((centroids.copy(), labels.copy()))\n",
    "\n",
    "    # Calculate variances for each cluster\n",
    "    cluster_var = [\n",
    "        np.sum((selected_data[labels == k] - centroids[k]) ** 2)\n",
    "        for k in range(n_clusters)\n",
    "    ]\n",
    "    cluster_variances.append(cluster_var)\n",
    "\n",
    "    # Recalculate centroids\n",
    "    new_centroids = np.array([selected_data[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
    "\n",
    "    # Check for convergence (if centroids do not change)\n",
    "    if np.all(centroids == new_centroids):\n",
    "        print(f\"Convergence reached at iteration {iteration + 1}\")\n",
    "        break\n",
    "\n",
    "    centroids = new_centroids\n",
    "\n",
    "# Visualization of clustering progression\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))  # Generate distinct colors for clusters\n",
    "\n",
    "for iteration, (centroids_iter, labels_iter) in enumerate(all_iterations):\n",
    "    # Create subplot for scatter plot and variance bar\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 12), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "    # Scatter plot\n",
    "    ax[0].scatter(selected_data[:, 0], selected_data[:, 1], c=labels_iter, cmap='viridis', alpha=0.6, s=50)\n",
    "    ax[0].scatter(centroids_iter[:, 0], centroids_iter[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "    ax[0].set_title(f'K-means Progression - Iteration {iteration + 1}')\n",
    "    ax[0].set_xlabel('PC1')\n",
    "    ax[0].set_ylabel('PC2')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Variance bar plot\n",
    "    variances = cluster_variances[iteration]\n",
    "    ax[1].bar(range(n_clusters), variances, color=colors, alpha=0.8)\n",
    "    ax[1].set_title('Cluster Variance Breakdown')\n",
    "    ax[1].set_xticks(range(n_clusters))\n",
    "    ax[1].set_xticklabels([f'Cluster {k}' for k in range(n_clusters)])\n",
    "    ax[1].set_ylabel('Variance (SSD)')\n",
    "    ax[1].set_xlabel('Clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
